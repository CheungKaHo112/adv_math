{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpHHlB4YU4yM",
        "outputId": "921ec2c8-c0e1-425e-a9b8-3b1b3179da4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "#check 123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RN9ABgqeU9Jq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "import tarfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hq3zgCuLVXB-"
      },
      "outputs": [],
      "source": [
        "def download_file(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(filename, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def extract_tarfile(filename, path):\n",
        "    with tarfile.open(filename, \"r:gz\") as tar_ref:\n",
        "        tar_ref.extractall(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YsCbmXGtVCJF"
      },
      "outputs": [],
      "source": [
        "# Add the download and extraction code from the previous answer\n",
        "\n",
        "data_url = \"http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\"\n",
        "filename = \"food-101.tar.gz\"\n",
        "\n",
        "download_file(data_url, filename)\n",
        "extract_tarfile(filename, \".\")\n",
        "\n",
        "# Prepare the dataset with only 3 classes and 500 images\n",
        "source_dir = \"food-101/images/\"\n",
        "dest_dir = \"reduced_food-101/\"\n",
        "\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "classes = os.listdir(source_dir)\n",
        "selected_classes = random.sample(classes, 3)\n",
        "\n",
        "for c in selected_classes:\n",
        "    os.makedirs(os.path.join(dest_dir, c), exist_ok=True)\n",
        "    class_images = os.listdir(os.path.join(source_dir, c))\n",
        "    selected_images = random.sample(class_images, min(500, len(class_images)))\n",
        "    for img in selected_images:\n",
        "        shutil.copy(os.path.join(source_dir, c, img), os.path.join(dest_dir, c, img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gAQ1AgDoVI42"
      },
      "outputs": [],
      "source": [
        "train_dir = \"train/\"\n",
        "val_dir = \"val/\"\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "for c in selected_classes:\n",
        "    os.makedirs(os.path.join(train_dir, c), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, c), exist_ok=True)\n",
        "    images = os.listdir(os.path.join(dest_dir, c))\n",
        "    train_images, val_images = train_test_split(images, test_size=0.2)\n",
        "\n",
        "    for img in train_images:\n",
        "        shutil.move(os.path.join(dest_dir, c, img), os.path.join(train_dir, c, img))\n",
        "    for img in val_images:\n",
        "        shutil.move(os.path.join(dest_dir, c, img), os.path.join(val_dir, c, img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8Aus3rMYowP",
        "outputId": "85d978da-79c5-4a41-e9c4-e8632059232d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1200 images belonging to 3 classes.\n",
            "Found 300 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (299, 299)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    fill_mode=\"nearest\",\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H28H6gnmYr5V",
        "outputId": "70f78170-65b5-4678-9064-3b21de504bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "# Create the base model\n",
        "base_model = Xception(\n",
        "    weights=\"imagenet\", include_top=False, input_shape=IMG_SIZE + (3,)\n",
        ")\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add a custom classification head\n",
        "inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(1024, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(len(selected_classes), activation=\"softmax\")(x)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRxy138jZRwm",
        "outputId": "0e0b9383-b27d-41bb-8f08-5caca0490781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "37/37 [==============================] - 463s 12s/step - loss: 0.2832 - accuracy: 0.8853 - val_loss: 0.1557 - val_accuracy: 0.9444\n",
            "Epoch 2/10\n",
            "37/37 [==============================] - 452s 12s/step - loss: 0.1358 - accuracy: 0.9469 - val_loss: 0.1224 - val_accuracy: 0.9444\n",
            "Epoch 3/10\n",
            "37/37 [==============================] - 455s 12s/step - loss: 0.1290 - accuracy: 0.9452 - val_loss: 0.0825 - val_accuracy: 0.9653\n",
            "Epoch 4/10\n",
            "37/37 [==============================] - 509s 14s/step - loss: 0.1002 - accuracy: 0.9649 - val_loss: 0.0884 - val_accuracy: 0.9583\n",
            "Epoch 5/10\n",
            "37/37 [==============================] - 463s 12s/step - loss: 0.0955 - accuracy: 0.9598 - val_loss: 0.1008 - val_accuracy: 0.9618\n",
            "Epoch 6/10\n",
            "37/37 [==============================] - 468s 13s/step - loss: 0.0752 - accuracy: 0.9735 - val_loss: 0.0827 - val_accuracy: 0.9688\n",
            "Epoch 7/10\n",
            "37/37 [==============================] - 471s 13s/step - loss: 0.0648 - accuracy: 0.9743 - val_loss: 0.0868 - val_accuracy: 0.9688\n",
            "Epoch 8/10\n",
            "37/37 [==============================] - 520s 14s/step - loss: 0.0782 - accuracy: 0.9700 - val_loss: 0.1418 - val_accuracy: 0.9514\n",
            "Epoch 9/10\n",
            "37/37 [==============================] - 471s 13s/step - loss: 0.0741 - accuracy: 0.9726 - val_loss: 0.0982 - val_accuracy: 0.9618\n",
            "Epoch 10/10\n",
            "37/37 [==============================] - 526s 14s/step - loss: 0.0793 - accuracy: 0.9700 - val_loss: 0.1040 - val_accuracy: 0.9618\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G1oA3CxZTO1",
        "outputId": "e02b8fae-0fee-4898-9dc5-cfb0246b7f32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20\n",
            "37/37 [==============================] - 1390s 37s/step - loss: 1.4679 - accuracy: 0.3433 - val_loss: 1.0986 - val_accuracy: 0.3368\n",
            "Epoch 11/20\n",
            "32/37 [========================>.....] - ETA: 2:50 - loss: 1.0990 - accuracy: 0.3165"
          ]
        }
      ],
      "source": [
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.0001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = EPOCHS + fine_tune_epochs\n",
        "\n",
        "history_fine_tune = model.fit(\n",
        "    train_generator,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=history.epoch[-1],\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOI5_ugQZVnM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
